{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the outcome of loan applications\n",
    "# 3b. Random Forest\n",
    "For this problem, we care about having a low number of false negatives as possible. False negatives, i.e. people we accept but should have rejected, pose a greater risk, because they could lead to loss of the capital lent as well as potential revenue from the interest. While high recall is associated to few false negatives, we can not tune model parameters by optimising on recall alone, otherwise the model will be pushed to behave like a random model, which has a recall of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "p = os.path.abspath('../')\n",
    "if p not in sys.path:\n",
    "    sys.path.append(p)\n",
    "\n",
    "from shared.data_processing import CategoricalEncoder\n",
    "from shared.data_processing import FeatureSelector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/loan_data_prepped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['label', 'accepted'], axis=1)\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the features\n",
    "Features of interest we have identified in the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERICAL_FEATURES = ['duration', 'loan_amount', 'age']\n",
    "\n",
    "FIXED_CATEGORICAL = ['foreign_worker_binary', 'checking_status_ordinal', 'savings_status_ordinal',\n",
    "                     'employment_ordinal', 'installment_commitment_ordinal']\n",
    "\n",
    "OTHER_CATEGORICAL = ['loan_history', 'purpose', 'other_parties', 'property_magnitude',\n",
    "                     'other_payment_plans', 'housing', 'personal_status', 'job']\n",
    "\n",
    "FEATURES = NUMERICAL_FEATURES + FIXED_CATEGORICAL + OTHER_CATEGORICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['duration',\n",
       " 'loan_amount',\n",
       " 'age',\n",
       " 'foreign_worker_binary',\n",
       " 'checking_status_ordinal',\n",
       " 'savings_status_ordinal',\n",
       " 'employment_ordinal',\n",
       " 'installment_commitment_ordinal',\n",
       " 'loan_history',\n",
       " 'purpose',\n",
       " 'other_parties',\n",
       " 'property_magnitude',\n",
       " 'other_payment_plans',\n",
       " 'housing',\n",
       " 'personal_status',\n",
       " 'job']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune model parameters\n",
    "We will focus on tuning the most important parameters, i.e. the maximum tree depth and the maximum number of features to use in each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CategoricalEncoder(features_to_encode=OTHER_CATEGORICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = FeatureSelector(features_to_select=NUMERICAL_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(class_weight='balanced', n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is not needed for a Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('encode', encoder),\n",
    "                       ('select', selector),\n",
    "                       ('forest', rfc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_coarse = {\n",
    "    'forest__criterion': ['gini', 'entropy'],\n",
    "    'forest__max_depth': [1, 5, 10, None],\n",
    "    'forest__max_features': [1, 5, 10, 'auto']\n",
    "}\n",
    "\n",
    "param_grid_fine = {\n",
    "    'forest__criterion': ['gini', 'entropy'],\n",
    "    'forest__max_depth': [3, 4, 5, 6, 7],\n",
    "    'forest__max_features': [1, 2, 3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe,\n",
    "                    verbose=1,\n",
    "                    cv=StratifiedKFold(n_splits=50),\n",
    "                    scoring=make_scorer(f1_score),\n",
    "                    param_grid=param_grid_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 50 folds for each of 30 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1500 out of 1500 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=50, random_state=None, shuffle=False),\n",
       "             estimator=Pipeline(steps=[('encode',\n",
       "                                        CategoricalEncoder(features_to_encode=['loan_history',\n",
       "                                                                               'purpose',\n",
       "                                                                               'other_parties',\n",
       "                                                                               'property_magnitude',\n",
       "                                                                               'other_payment_plans',\n",
       "                                                                               'housing',\n",
       "                                                                               'personal_status',\n",
       "                                                                               'job'])),\n",
       "                                       ('select',\n",
       "                                        FeatureSelector(features_to_select=['duration',\n",
       "                                                                            'loan_amount',\n",
       "                                                                            'age'])),\n",
       "                                       ('forest',\n",
       "                                        RandomForestClassifier(class_weight='balanced'))]),\n",
       "             param_grid={'forest__criterion': ['gini', 'entropy'],\n",
       "                         'forest__max_depth': [3, 4, 5, 6, 7],\n",
       "                         'forest__max_features': [1, 2, 3]},\n",
       "             scoring=make_scorer(f1_score), verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy\n",
      "4\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "best_criterion = grid.best_estimator_.get_params()['forest__criterion']\n",
    "best_max_depth = grid.best_estimator_.get_params()['forest__max_depth']\n",
    "best_max_features = grid.best_estimator_.get_params()['forest__max_features']\n",
    "\n",
    "print(best_criterion)\n",
    "print(best_max_depth)\n",
    "print(best_max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the best model over 50 random splits\n",
    "Given that the data set is so small, it's important to evaluate over many random train/test splits, so that we get a better picture of the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_tuned = RandomForestClassifier(class_weight='balanced',\n",
    "                                   n_estimators=100,\n",
    "                                   criterion=best_criterion,\n",
    "                                   max_depth=best_max_depth,\n",
    "                                   max_features=best_max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_tuned = Pipeline(steps=[('encoder', encoder),\n",
    "                             ('select', selector),\n",
    "                             ('forest', rfc_tuned)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=50, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_FUNCTIONS = {\n",
    "    'accuracy': accuracy_score,\n",
    "    'precision': precision_score,\n",
    "    'recall': recall_score,\n",
    "    'f1': f1_score\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = {k: [] for k in METRIC_FUNCTIONS.keys()}\n",
    "\n",
    "for train_IDX, test_IDX in sss.split(X, y):\n",
    "    pipe_tuned.fit(X.loc[train_IDX], y.loc[train_IDX])\n",
    "    predictions = pipe_tuned.predict(X.loc[test_IDX])\n",
    "    truth = y.loc[test_IDX]\n",
    "    \n",
    "    for key, metric in METRIC_FUNCTIONS.items():\n",
    "        METRICS[key].append(metric(truth, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.64048,\n",
       " 'precision': 0.42180782208104367,\n",
       " 'recall': 0.5253333333333333,\n",
       " 'f1': 0.46635866220578265}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: np.mean(v) for k, v in METRICS.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.030804051681556453,\n",
       " 'precision': 0.038858188172290434,\n",
       " 'recall': 0.06510162995057975,\n",
       " 'f1': 0.04286913036061381}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: np.std(v) for k, v in METRICS.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
